{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-05T20:19:54.536145",
     "start_time": "2017-02-05T20:19:53.890239"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# read schema \n",
    "schema_rawfile = pd.read_csv(\"twitter-swisscom/schema_home.txt\", header=None, sep='\\s+')\n",
    "# extract schema columns\n",
    "data_columns = schema_rawfile[1].values\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-29T16:16:59.297297",
     "start_time": "2017-01-29T16:16:59.285219"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_list = [0, 3, 10, 11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emoji\n",
    "\n",
    "**Handcrafted two sets of Emojis Happy vs Sad**\n",
    "\n",
    "Compile the unicode of those Emojis in regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-29T16:17:00.172184",
     "start_time": "2017-01-29T16:17:00.154977"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emoji_happy = re.compile(\n",
    "    u\"(\\U0001f60a)|\"  # üòä\n",
    "    u\"(\\u2764\\ufe0ff)|\"  # ‚ù§Ô∏è\n",
    "    u\"(\\u2764)|\"  # ‚ù§\n",
    "    u\"(\\U0001f601)|\"  # üòä\n",
    "    u\"(\\U0001f600)|\"  # üòÄ\n",
    "    u\"(\\U0001f602)|\"  # üòÅ\n",
    "    u\"(\\U0001f609)|\"  # üòâ\n",
    "    u\"(\\U0001f60b)|\"  # üòã\n",
    "    u\"(\\U0001f60e)|\"  # üòé\n",
    "    u\"(\\U0001f618)|\"  # üòò\n",
    "    u\"(\\U0001f60d)|\"  # üòç\n",
    "    u\"(\\U0001f617)|\"  # üòó\n",
    "    u\"(\\u263a\\ufe0f)\"  # ‚ò∫Ô∏è\n",
    "    \"+\", flags=re.UNICODE)\n",
    "\n",
    "emoji_sad = re.compile(\n",
    "    u\"(\\U0001f610)|\"  # \n",
    "    u\"(\\U0001f61e)|\"  # \n",
    "    u\"(\\U0001f61f)|\"  # \n",
    "    u\"(\\U0001f622)|\"  # \n",
    "    u\"(\\U0001f624)|\"  # \n",
    "    u\"(\\U0001f62d)|\"  # \n",
    "    u\"(\\U0001f629)|\"  # \n",
    "    u\"(\\U0001f628)|\"  # \n",
    "    u\"(\\U0001f620)|\"  # \n",
    "    u\"(\\U0001f621)|\"  # \n",
    "    u\"(\\U0001f635)|\"  # \n",
    "    u\"(\\U0001f631)|\"  # \n",
    "    u\"(\\U0001f611)|\"  # \n",
    "    u\"(\\U0001f60f)|\"  #\n",
    "    u\"(\\U0001f623)|\"  # \n",
    "    u\"(\\U0001f625)|\"  # \n",
    "    u\"(\\U0001f910)|\"  # \n",
    "    u\"(\\U0001f62b)|\"  # \n",
    "    u\"(\\U0001f612)|\"  # \n",
    "    u\"(\\U0001f613)|\"  # \n",
    "    u\"(\\U0001f614)|\"  # \n",
    "    u\"(\\u2639\\ufe0f)|\" # \n",
    "    u\"(\\U0001f641)|\"  # \n",
    "    u\"(\\U0001f616)\"  # \n",
    "    \"+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emoticons\n",
    "\n",
    "regex to match and extract happy and sad emoticons from text.\n",
    "\n",
    "**Happy: \n",
    " :p \n",
    " :3 \n",
    " :* \n",
    " :)) \n",
    " (: \n",
    " :-) \n",
    " :-P \n",
    " :-p \n",
    " :-3 \n",
    " :-* \n",
    " :^) \n",
    " :^P \n",
    " :^p \n",
    " :^3 \n",
    " :^* \n",
    " (: \n",
    " (-: \n",
    " (^: < 3**\n",
    " \n",
    " **Sad:\n",
    " :O\n",
    " :|\n",
    " :/\n",
    " :\\**\n",
    " :$\n",
    " :((\n",
    " :-(\n",
    " :-O\n",
    " :-|\n",
    " :-/\n",
    " :-\\\n",
    " :-$\n",
    " :^(\n",
    " :^O\n",
    " :^|\n",
    " :^/\n",
    " :^\\\n",
    " :^$\n",
    " ):\n",
    " )-:\n",
    " )^:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-29T16:17:01.083107",
     "start_time": "2017-01-29T16:17:01.074432"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emoticon_happy = re.compile(\"(\\:\\w+\\:|\\^\\^|\\<[\\/\\\\]?3|[\\(|][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DPp\\*\\)]+)(?=\\s|[\\!\\.\\?]|$)\")\n",
    "emoticon_sad = re.compile(\"(\\:\\w+\\:|[\\)][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[O\\$\\\\\\(\\/\\|]+)(?=\\s|[\\!\\.\\?]|$)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Happy and Sad Tweets\n",
    "\n",
    "The extract_tweets function first strip and sanitize the tweets in dataframe, then extract and save the tweets to text files based on emoji/emoticon regex matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-29T16:18:02.104317",
     "start_time": "2017-01-29T16:18:02.051901"
    },
    "collapsed": false,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def extract_tweets(df):\n",
    "    NaN_text = df.text.isnull()\n",
    "    df = df[~ NaN_text]\n",
    "\n",
    "    happy_tweets = \"\"\n",
    "    sad_tweets = \"\"\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # match url in string and replace with \"\" empty string\n",
    "        t = re.sub(r\"http\\S+\", \" \", row['text'])\n",
    "        # remove @username\n",
    "        t = re.sub('@[^\\s]+', \" \", t)\n",
    "        # find all hashtags\n",
    "        tag_list = re.findall(r'#(\\w+)', t)\n",
    "        # remove hashtags\n",
    "        t = re.sub(r'#(\\w+)', \" \",  t)\n",
    "\n",
    "        if (emoji_happy.search(t) or emoticon_happy.search(t)):\n",
    "            t = emoticon_happy.sub(' ', t)\n",
    "            t = emoji_happy.sub(' ', t)\n",
    "            t = emoticon_sad.sub(' ', t)\n",
    "            t = emoji_sad.sub(' ', t)\n",
    "            happy_tweets += (t + '\\n')\n",
    "            \n",
    "        if (emoji_sad.search(t) or emoticon_sad.search(t)):\n",
    "            t = emoticon_sad.sub(' ', t)\n",
    "            t = emoji_sad.sub(' ', t)\n",
    "            t = emoticon_happy.sub(' ', t)\n",
    "            t = emoji_happy.sub(' ', t)\n",
    "            sad_tweets += (t + '\\n')\n",
    "    # print(happy_tweets)\n",
    "    # print(\"========================================================================\")\n",
    "    # print(sad_tweets)\n",
    "    \n",
    "    text_file = open(\"happy_tweets_line.txt\", \"a\")\n",
    "    text_file.write(happy_tweets)\n",
    "    text_file.close()\n",
    "    \n",
    "    text_file2 = open(\"sad_tweets_line.txt\", \"a\")\n",
    "    text_file2.write(sad_tweets)\n",
    "    text_file2.close()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-29T17:12:56.051885",
     "start_time": "2017-01-29T16:18:16.113236"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel/__main__.py:1: DeprecationWarning: 'U' mode is deprecated\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# Apply the extract_tweets function on the 26 million tweet data, we decide to chunk the data in 2000 pieces.\n",
    "for data in pd.read_table(open(\"data_clean/twex.tsv\", 'rU'),sep='\\t',encoding='utf-8',escapechar=\"\\\\\",na_values='N', index_col=0,quoting=csv.QUOTE_NONE, header=None, names=data_columns, chunksize=10000, engine='c', usecols=col_list):\n",
    "    extract_tweets(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-05T20:18:37.929983",
     "start_time": "2017-02-05T20:18:37.089974"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel/__main__.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Check if the tweets have been extracted correctly.\n",
    "sad_line = pd.read_csv(\"sad_tweets_line.txt\", header=None, sep='\\\\n', names=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-05T20:18:39.873051",
     "start_time": "2017-02-05T20:18:39.813209"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>leider werden die rumfahrenden nicht verschrot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bye bye 4-hour Workweek   back to 9-5 tomorrow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cashier in the Apple Store: \"This (adaptor) is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sometimes I don't quite live up to my gadgetee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>There are strange days. This was one of them.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sorry - heute ganzer Tag Workshop im Software ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hm, die grosse Einkaufstour bleibt f√§llt wohl ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Danke f√ºr alle die mich unterst√ºtzt haben! Es ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Obligatoire pour son Iphone (juste apr√®s talki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Argh, das ist ja √§rgerlich   Wie siehts beim M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I don't mean to be rude, but: Updates fail and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Wow che consumi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Lugano‚ÄîZ√ºrich mit 1 Stunde Versp√§tung und 2 ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ich nutze es t√§glich, aber da scheint wohl nie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Traffic jam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ein Insider (Programmierer) meint: Ja. Und wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mmhh get a sheet of paper and a pen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Pollution sonore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>And back to the parking space...   technical p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Rentre d' une soir√©e ann√©es 80' c √©tait bien c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>marche pas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Hilfe, ich sitz bei schlechtem Wetter in Affei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>I want it but it will be available in Zwitzerl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Volcanic ash from Iceland!!! WTF hopefully I'l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>I paid for it 2 weeks ago   RT   Qik Video Cam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>haha! je sais dessiner Hitler en emoticons!!  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>good luck man!!! My flight was cancelled as well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ich liebe hanuta, immer wenn ich im z√ºri offic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>sorry, ich pack es nicht. Bis grad im Office u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Not too much time for Twitter lately...    Rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235863</th>\n",
       "      <td>die Frage ist nur, wie bekomme ich die hundert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235864</th>\n",
       "      <td>A√ß kapƒ±yƒ± veysel efendi..Damat Ferit gidiyor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235865</th>\n",
       "      <td>Heading to   ... with an umbrella</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235866</th>\n",
       "      <td>Liada Die Post y, por primera vez desde que te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235867</th>\n",
       "      <td>Odio infinito a esa gente que hace da√±o utiliz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235868</th>\n",
       "      <td>sale malade va</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235869</th>\n",
       "      <td>jsais pas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235870</th>\n",
       "      <td>Oh si, el oto√±o ya est√° aqu√≠   @ Colombier, Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235871</th>\n",
       "      <td>.  on 'Apache  '</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235872</th>\n",
       "      <td>(  platform for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235873</th>\n",
       "      <td>&amp;gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235874</th>\n",
       "      <td>c elle qui fait que rater son permis aussi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235875</th>\n",
       "      <td>mddr c la v√©rit√© aussi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235876</th>\n",
       "      <td>ui mddr mais c le bordel faut des heures de n√©...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235877</th>\n",
       "      <td>HO FOTTUTAMENTE FREDDO.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235878</th>\n",
       "      <td>unfortunately not   I will give a shorter vers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235879</th>\n",
       "      <td>So so so so -</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235880</th>\n",
       "      <td>leider nicht die 42mm, Aluminium schwarz, Spor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235881</th>\n",
       "      <td>Sven Crone on survey of statistical forecastin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235882</th>\n",
       "      <td>usage (e.g. averages  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235883</th>\n",
       "      <td>\"mulhouse south alsace\" ptdr le filtre snap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235884</th>\n",
       "      <td>had that Problem as well once (with DLCs). It ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235885</th>\n",
       "      <td>le voisin de la meuf √©bahie par le train est m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235886</th>\n",
       "      <td>d'apr√®s une enqu√™te men√©e dans des centres urb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235887</th>\n",
       "      <td>hey   I use a swiss keyboard layout where the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235888</th>\n",
       "      <td>J√ºrgen Schmidhuber on the father and the origi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235889</th>\n",
       "      <td>J'veux retourner √† Milan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235890</th>\n",
       "      <td>Never happened to me before.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235891</th>\n",
       "      <td>it is   this Monday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235892</th>\n",
       "      <td>non le design lol apr√®s la qualit√© l√† on peut ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>235893 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text\n",
       "0       leider werden die rumfahrenden nicht verschrot...\n",
       "1       bye bye 4-hour Workweek   back to 9-5 tomorrow...\n",
       "2       Cashier in the Apple Store: \"This (adaptor) is...\n",
       "3       Sometimes I don't quite live up to my gadgetee...\n",
       "4           There are strange days. This was one of them.\n",
       "5       sorry - heute ganzer Tag Workshop im Software ...\n",
       "6       Hm, die grosse Einkaufstour bleibt f√§llt wohl ...\n",
       "7       Danke f√ºr alle die mich unterst√ºtzt haben! Es ...\n",
       "8       Obligatoire pour son Iphone (juste apr√®s talki...\n",
       "9       Argh, das ist ja √§rgerlich   Wie siehts beim M...\n",
       "10      I don't mean to be rude, but: Updates fail and...\n",
       "11                                        Wow che consumi\n",
       "12      Lugano‚ÄîZ√ºrich mit 1 Stunde Versp√§tung und 2 ma...\n",
       "13      ich nutze es t√§glich, aber da scheint wohl nie...\n",
       "14                                            Traffic jam\n",
       "15      Ein Insider (Programmierer) meint: Ja. Und wil...\n",
       "16                 mmhh get a sheet of paper and a pen...\n",
       "17                                       Pollution sonore\n",
       "18      And back to the parking space...   technical p...\n",
       "19      Rentre d' une soir√©e ann√©es 80' c √©tait bien c...\n",
       "20                                             marche pas\n",
       "21      Hilfe, ich sitz bei schlechtem Wetter in Affei...\n",
       "22      I want it but it will be available in Zwitzerl...\n",
       "23      Volcanic ash from Iceland!!! WTF hopefully I'l...\n",
       "24      I paid for it 2 weeks ago   RT   Qik Video Cam...\n",
       "25      haha! je sais dessiner Hitler en emoticons!!  ...\n",
       "26       good luck man!!! My flight was cancelled as well\n",
       "27      ich liebe hanuta, immer wenn ich im z√ºri offic...\n",
       "28      sorry, ich pack es nicht. Bis grad im Office u...\n",
       "29      Not too much time for Twitter lately...    Rea...\n",
       "...                                                   ...\n",
       "235863  die Frage ist nur, wie bekomme ich die hundert...\n",
       "235864       A√ß kapƒ±yƒ± veysel efendi..Damat Ferit gidiyor\n",
       "235865                  Heading to   ... with an umbrella\n",
       "235866  Liada Die Post y, por primera vez desde que te...\n",
       "235867  Odio infinito a esa gente que hace da√±o utiliz...\n",
       "235868                                     sale malade va\n",
       "235869                                          jsais pas\n",
       "235870  Oh si, el oto√±o ya est√° aqu√≠   @ Colombier, Ne...\n",
       "235871                                   .  on 'Apache  '\n",
       "235872                                    (  platform for\n",
       "235873                                               &gt;\n",
       "235874         c elle qui fait que rater son permis aussi\n",
       "235875                             mddr c la v√©rit√© aussi\n",
       "235876  ui mddr mais c le bordel faut des heures de n√©...\n",
       "235877                            HO FOTTUTAMENTE FREDDO.\n",
       "235878  unfortunately not   I will give a shorter vers...\n",
       "235879                                      So so so so -\n",
       "235880  leider nicht die 42mm, Aluminium schwarz, Spor...\n",
       "235881  Sven Crone on survey of statistical forecastin...\n",
       "235882                          usage (e.g. averages  ...\n",
       "235883        \"mulhouse south alsace\" ptdr le filtre snap\n",
       "235884  had that Problem as well once (with DLCs). It ...\n",
       "235885  le voisin de la meuf √©bahie par le train est m...\n",
       "235886  d'apr√®s une enqu√™te men√©e dans des centres urb...\n",
       "235887  hey   I use a swiss keyboard layout where the ...\n",
       "235888  J√ºrgen Schmidhuber on the father and the origi...\n",
       "235889                           J'veux retourner √† Milan\n",
       "235890                       Never happened to me before.\n",
       "235891                                it is   this Monday\n",
       "235892  non le design lol apr√®s la qualit√© l√† on peut ...\n",
       "\n",
       "[235893 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sad_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets extraction Conclusion\n",
    "\n",
    "**happy tweets extracted**\n",
    "\n",
    "76M\n",
    "\n",
    "‚Üí wc -l happy_tweets_line.txt\n",
    "  1767108 happy_tweets_line.txt\n",
    "\n",
    "**sad tweets extracted**\n",
    "\n",
    "12M\n",
    "\n",
    "‚Üí wc -l sad_tweets_line.txt\n",
    "  256019 sad_tweets_line.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-05T20:19:59.449928",
     "start_time": "2017-02-05T20:19:59.353383"
    },
    "code_folding": [
     2
    ],
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'happy_lexicon' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-86cb3ee6aecb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhappy_lexicon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msad_lexicon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'happy_lexicon' is not defined"
     ]
    }
   ],
   "source": [
    "# Accuracy testing\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for index, row in sad_line.iterrows():\n",
    "    score = 0\n",
    "        \n",
    "        # match url in string and replace with \"\" empty string\n",
    "    t = re.sub(r\"http\\S+\", \" \", row['text'])\n",
    "        # remove @username\n",
    "    t = re.sub('@[^\\s]+', \" \", t)\n",
    "        # remove hashtags\n",
    "    t = re.sub(r'#(\\w+)', \" \",  t)\n",
    "\n",
    "    tbl = str.maketrans({ord(ch):\" \" for ch in punctuation})\n",
    "        \n",
    "    tokens = t.translate(tbl).split()\n",
    "            \n",
    "    for it in tokens:\n",
    "        if (it in happy_lexicon):\n",
    "            score += 1\n",
    "        if (it in sad_lexicon):\n",
    "            score -= 1\n",
    "    \n",
    "    if(score > 0):\n",
    "        counter = (counter + 1)\n",
    "\n",
    "print(counter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
