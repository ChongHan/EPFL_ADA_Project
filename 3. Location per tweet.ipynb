{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the location information we need to the visualizations, we will used the *district* method from the previous method. The idea is to compute the nearest district capital for each tweet, and based on this value, we can also retrieve the tweet location canton.\n",
    "\n",
    "We have choosen to run this code on a \"simple\" laptop. It took a full day and a half to run it for the entire dataset. Since python is not multicore, we run two instance of the scipt at the same time, each one with a half of the data. Each chunk of the data save its result in a csv file, that we append all together after the computation was done.\n",
    "\n",
    "Note that we decided to not us Spark for time consideration. Indeed, it would have probably requires us at least half a day to understand how to use our code with Spark and Hadoop. But in the middle of the exam session, every second counts! Therefore, we decided to let the computer run for a day while we prepare less funny exams.\n",
    "\n",
    "The python scipt we used is copy/paste on the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import folium\n",
    "import json\n",
    "import pickle\n",
    "from geopy.distance import vincenty\n",
    "\n",
    "\n",
    "#\n",
    "## First: merge the districts and canton information\n",
    "#\n",
    "\n",
    "\n",
    "# Load the districts information\n",
    "df_districts = pd.read_table('data/ch_districts_capital.txt', sep=',')\n",
    "df_districts.index = df_districts.id_ofs\n",
    "\n",
    "# Load the cities data, containing the canton of each city\n",
    "switzerland_cities = pd.read_csv(\"data/switzerland_cities.txt\")\n",
    "switzerland_cities = switzerland_cities.sort_values('Population', ascending=False)\n",
    "\n",
    "# Combine both datasets\n",
    "district_idToLocation = list()\n",
    "\n",
    "for i,district in df_districts.iterrows():\n",
    "    capital_location = switzerland_cities[switzerland_cities['ASCII City Name'] == district.capital.lower()]\n",
    "    district_location = str(capital_location.Longitude.values[0]) + \",\" + str(capital_location.Latitude.values[0])\n",
    "    district_idToLocation.append([[district.id_ofs,district.canton], district_location])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "## Now we can deal with the tweets\n",
    "#\n",
    "\n",
    "\n",
    "# Tweets data schema_rawfile\n",
    "schema_rawfile = pd.read_csv(\"twitter-swisscom/schema_home.txt\", header=None, sep='\\s+')\n",
    "data_columns = schema_rawfile[1].values\n",
    "\n",
    "\n",
    "def findLocation(x):\n",
    "    \"\"\"\n",
    "        Return the location of the tweet. The location is a string composed of the longitude and the latitude.\n",
    "        The coordinates are found out by the 'longitude' and 'latitude' columns of the data. If these values are null, \n",
    "        we try to retrieve the location through the 'placeLongitude' and 'placeLatitude' columns.\n",
    "        If this second location is also null, we return a null locaton (\"nan, nan\")\n",
    "\n",
    "        INPUT\n",
    "                x:      A row of the tweets data\n",
    "\n",
    "        OUTPUT\n",
    "                loc1:   The longitude-latitude tuple, or 'nan,nan' is case of null location\n",
    "    \"\"\"\n",
    "    loc1 = str(x.longitude) + \",\" + str(x.latitude)\n",
    "    if (loc1 == \"nan,nan\" or loc1 == r\"\\N,\\N\"):\n",
    "        loc1 = str(x.placeLongitude) + \",\" + str(x.placeLatitude)\n",
    "    if (loc1 == r\"\\N,\\N\"):\n",
    "        loc1 = \"nan,nan\"\n",
    "    return loc1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def findNearestCity(x):\n",
    "    \"\"\"\n",
    "        Return the district capital which is nearest to the tweet location. \n",
    "        If the nearest district capital is more that 50 kilometers way, return a NULL value\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the nearest district capital\n",
    "    min_city = min(district_idToLocation, key=lambda x: vincenty(x[1], row.Location).miles)\n",
    "\n",
    "    # Assert the distance is smaller that 50 km\n",
    "    min_distance = vincenty(min_city[1],row.Location).km\n",
    "    if min_distance > 50:\n",
    "        return \"NULL\"               # Foreign tweet\n",
    "    else:\n",
    "        return min_city[0]          # All good baby !\n",
    "\n",
    "\n",
    "\n",
    "# Variables used in the 'for loop'\n",
    "i = 0                                   # the current chunk\n",
    "removedTot=0                            # Number of removed tweets with a null location\n",
    "removedIds = list()                     # List of the index of the removed tweets with a null location\n",
    "removedRows = 0                         # Number of removed tweets by an invalid index\n",
    "removedRowsIds = list()                 # List of the index of the removed tweets with an invalid location\n",
    "\n",
    "\n",
    "usecoll = ['id','text','longitude','latitude','placeLongitude','placeLatitude']    # Columns to load with the data\n",
    "for data in pd.read_table(open(\"twitter-swisscom/twex.tsv\", 'rU'),sep='\\t',encoding='utf-8',escapechar=\"\\\\\",na_values='N', index_col=0,quoting=csv.QUOTE_NONE, header=None, names=data_columns, chunksize=10000, engine='c', usecols=usecoll):\n",
    "\n",
    "    # Range of chunks to take \"jump\"\n",
    "    if (i > 1000):\n",
    "        if (i%10 == 0):\n",
    "            print(\"---------------\",i)\n",
    "        i += 1\n",
    "        continue\n",
    "    \n",
    "\n",
    "\n",
    "    # Compute the location information for each tweet\n",
    "    data['Location'] = data.apply(findLocation, axis=1)\n",
    "    a = data.Location == \"nan,nan\"                                                  # If we weren't able to retrieve the location information, we can't use those tweets => remove them\n",
    "    removedTot += len(data[a])\n",
    "    removedIds.append(data[a].index)\n",
    "    data = data[data.Location != \"nan,nan\"]                                         # Update data\n",
    "\n",
    "    \n",
    "    # Assert the validity of the tweets by asserting that the index is a number\n",
    "    data['idx'] = data.index\n",
    "    data['isIdxValid'] = data.apply(lambda row: str(row.idx).isdigit(), axis=1)\n",
    "    removedRows += (data.isIdxValid == False).sum()                                 # If the index is not valid, remove those tweets as well\n",
    "    removedRowsIds.append(data[data.isIdxValid == False].index)\n",
    "    data = data[data.isIdxValid == True]                                            # Update date\n",
    "\n",
    "    if (len(data) == 0):\n",
    "        i += 1\n",
    "        continue\n",
    "\n",
    "    \n",
    "    # Compute the closes district\n",
    "    data['LocInfo'] = data.apply(findNearestCity, axis=1)\n",
    "    data = data[data.LocInfo != \"NULL\"]                                             # Remove the \"foreign\" tweets\n",
    "\n",
    "    data['District'] = data.apply(lambda r: r.LocInfo[0], axis=1)\n",
    "    data['Canton'] = data.apply(lambda r: r.LocInfo[1], axis=1)\n",
    "    \n",
    "\n",
    "    # Export the triplete \"tweet id\"+\"District\"+\"Canton\"\n",
    "    data_to_export = data[['District', 'Canton']]\n",
    "    name = 'data_district_canton/data_'+str(i)+'.csv'\n",
    "    data_to_export.to_csv(name, header=False)\n",
    "    \n",
    "    \n",
    "    # Go to the next chunk\n",
    "    if (i%10 == 0):\n",
    "        print(\"---------------\",i)\n",
    "    i+=1\n",
    "\n",
    "\n",
    "\n",
    "print(\"DONE\")\n",
    "print(\"RemovedTot: \",removedTot)\n",
    "print(\"RemovedIDS: \", removedIds)\n",
    "\n",
    "\n",
    "filehandler = open(\"out/removedIds\",\"wb\")\n",
    "pickle.dump(removedIds,filehandler)\n",
    "filehandler.close()\n",
    "\n",
    "filehandler = open(\"out/removedRowsIds\",\"wb\")\n",
    "pickle.dump(removedRowsIds,filehandler)\n",
    "filehandler.close()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": "0",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
