{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project with to compare tweets sentiment score with their location and time. Therefore, we need to be able to give some *time* information about each tweet. That the purpose of this code.\n",
    "\n",
    "Each tweet has a time information in its **createdAt** column. This column contains strings that we can easily transform to a *datetime* object. From that object, we retrieve the **year, month, ** and **day** for the tweets. We only do it for tweets who are valid (meaning their index is an integer), and who are also present in the *location dataset* (because we will need to join everything after, so no need to deal with tweets that we already know to be uselesss).\n",
    "\n",
    "Below is the python script we launch to create a csv file for each chunk of data. Once the script was done, we merge all these files. The csv file create are structured like : **Tweet id, Year, Month, Day**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "## Merge the District and Canton information\n",
    "#\n",
    "\n",
    "# Load the districts informaiton\n",
    "df_districts = pd.read_table('data/ch_districts_capital.txt', sep=',')\n",
    "df_districts.index = df_districts.id_ofs\n",
    "\n",
    "# Load the cities data, containing the canton of each city\n",
    "switzerland_cities = pd.read_csv(\"data/switzerland_cities.txt\")\n",
    "switzerland_cities = switzerland_cities.sort_values('Population', ascending=False)\n",
    "\n",
    "# Combine districts with their appropriate canton\n",
    "schema_rawfile = pd.read_csv(\"twitter-swisscom/schema_home.txt\", header=None, sep='\\s+')\n",
    "data_columns = schema_rawfile[1].values\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "## Retrieve the dates infromations for each tweet\n",
    "#\n",
    "\n",
    "\n",
    "# Variable used inside the for-loop loading the data\n",
    "i = 0                                                                                                   # Current chunk of data\n",
    "usecolumns = ['id', 'createdAt']                                                                        # The only columns of interest for retrieve the date information we want\n",
    "data_loc = pd.read_csv(\"data_created/data_withLocation.csv\", index_col=0, names=['District', 'Canton'])         # Load the id+Canton+District computed previously. \n",
    "\n",
    "# Load the data in chunks\n",
    "for data in pd.read_table(open(\"twitter-swisscom/twex.tsv\", 'r'), sep='\\t', escapechar=\"\\\\\", na_values='N', encoding='utf-8', quoting=csv.QUOTE_NONE, header=None, names=data_columns, engine='c', usecols=usecolumns, chunksize=10000 ):\n",
    "    \n",
    "    # Remove all entries with an invalid id. A valid id must be an integer.\n",
    "    data = data[~data.id.isnull()] \n",
    "    data['isIdxValid'] = data.apply(lambda row: str(row.id).isdigit(), axis=1)\n",
    "    data = data[data.isIdxValid == True]\n",
    "    data.id = data.id.astype('int64')\n",
    "    \n",
    "    # If we already remove all entries of the chunk, go to the next one\n",
    "    if (len(data) == 0):\n",
    "        i +=1\n",
    "        continue\n",
    "    \n",
    "\n",
    "    # Keep the one for which we have the location info only. Useless to compute the date for tweets we don't have the location\n",
    "    data['idOK'] = data.apply(lambda row: row.id in data_loc.index, axis=1)\n",
    "    data = data[data.idOK]\n",
    "    \n",
    "    \n",
    "    # If we still have some entries in this chunk of data, let's retriveve the data information we want.\n",
    "    if (len(data) != 0):\n",
    "        \n",
    "        # Set id\n",
    "        data.index = data.id\n",
    "        \n",
    "        # Remove the ones without the date info\n",
    "        data['hasDate'] = data.createdAt != '0000-00-00 00:00:00'\n",
    "        data = data[data.hasDate]\n",
    "        \n",
    "        # Convert the type\n",
    "        data['date'] = pd.DatetimeIndex(data['createdAt']).normalize()\n",
    "        \n",
    "        # Retrieve the required date informaiton\n",
    "        data['Year'] = pd.DatetimeIndex(data['date']).year\n",
    "        data['Month'] = pd.DatetimeIndex(data['date']).month\n",
    "        data['Day'] = pd.DatetimeIndex(data['date']).day\n",
    "        \n",
    "        \n",
    "        # Export: \"Tweet id\" + \"Year\" + \"Month\" + \"Day\"\n",
    "        data_to_export = data[['Year','Month','Day']]\n",
    "        name = 'data_dates/data_'+str(i)+'.csv'\n",
    "        data_to_export.to_csv(name, header=False)\n",
    "    \n",
    "    \n",
    "    # Move to the next chunk\n",
    "    i += 1\n",
    "    if i%10==0: print(i)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": "0",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
